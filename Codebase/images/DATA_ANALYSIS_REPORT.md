# üìä DATA ANALYSIS REPORT - ASPECT-BASED SENTIMENT TRIPLET EXTRACTION

## 1. T√ìNG QUAN D·ªÆ LI·ªÜU (Overview)

### Dataset Statistics
```
Total sentences:  5,989
Total entities:  20,504 (avg 3.42 per sentence)
Total triplets:  10,252 (avg 1.71 per sentence)
Entity-to-Triplet ratio: 2:1 (m·ªói triplet c√≥ 2 th·ª±c th·ªÉ: aspect + opinion)
```

### Dataset Breakdown
| Dataset | Train | Dev | Test | Total |
|---------|-------|-----|------|-------|
| **14lap** (Laptops) | 906 | 219 | 328 | 1,453 |
| **14res** (Restaurants) | 1,266 | 310 | 492 | 2,068 |
| **15res** (Restaurants) | 605 | 148 | 322 | 1,075 |
| **16res** (Restaurants) | 857 | 210 | 326 | 1,393 |

---

## 2. PH√ÇN T√çCH CHI TI·∫æT (Detailed Analysis)

### A. Sentiment Distribution (Ph√¢n b·ªë c·∫£m x√∫c)

**Problem:** M·∫•t c√¢n b·∫±ng d·ªØ li·ªáu L·ªöNNN! ‚ö†Ô∏è
```
Positive (POSITIVE): 57.5% - 74.5%  (chi·∫øm ∆∞u th·∫ø)
Negative (NEGATIVE): 19.3% - 33.0%  (thi·ªÉu s·ªë)
Neutral  (NEUTRAL):  3.5% - 9.6%    (r·∫•t hi·∫øm)
```

**Nguy√™n nh√¢n:** D·ªØ li·ªáu review t·ª± nhi√™n ‚Üí ng∆∞·ªùi d√πng th∆∞·ªùng review khi r·∫•t h√†i l√≤ng ho·∫∑c r·∫•t kh√¥ng h√†i l√≤ng, √≠t neutral.

**·∫¢nh h∆∞·ªüng:**
- Model d·ªÖ b·ªã overfitting v√†o POSITIVE
- Hi·ªáu su·∫•t tr√™n NEGATIVE & NEUTRAL k√©m
- F1-score kh√¥ng ph·∫£n √°nh ƒë√∫ng ch·∫•t l∆∞·ª£ng

### B. Entity Characteristics (ƒê·∫∑c ƒëi·ªÉm th·ª±c th·ªÉ)

**Entity Span Length Distribution:**
```
1 token:  74.5% - 82.4% (h·∫ßu h·∫øt l√† single-word)
2 tokens: 12-19%        (short phrases)
3+ tokens: < 5%         (multi-word entities hi·∫øm)
```

**√ù nghƒ©a:**
- Th·ª±c th·ªÉ ch·ªß y·∫øu l√† t·ª´ ƒë∆°n (adjectives, nouns)
- VD: "good", "battery", "fast", "screen"
- √çt multi-word expressions: "battery life", "customer service"

### C. Triplet Patterns (M·∫´u triplet)

**Pattern Structure:**
```
Triplet = (aspect/target, opinion, sentiment_type)
```

**M·∫´u ch√≠nh (99%+):**
- target ‚Üí opinion (aspect points to opinion word)
- Lu√¥n l√† c·∫∑p aspect-opinion, KH√îNG khi n√†o l√† opinion-target

**Multiplicity (S·ªë triplet m·ªói c√¢u):**
- Trung b√¨nh: 1.6-2.0 triplet/c√¢u
- M·ªôt s·ªë c√¢u c√≥ 5-6 triplet (multi-aspect multi-opinion)
- ƒêi·ªÅu n√†y l√†m tƒÉng ƒë·ªô kh√≥: ph·∫£i tr√≠ch xu·∫•t ch√≠nh x√°c to√†n b·ªô m·ªëi quan h·ªá

---

## 3. L√ÄM TH·∫æ N√ÄO C√ì TH·ªÇ C·∫¢I THI·ªÜN M√î H√åNH (Improvement Ideas)

### üéØ Chi·∫øn l∆∞·ª£c 1: X·ª≠ l√Ω M·∫•t C√¢n B·∫±ng D·ªØ Li·ªáu

#### A. Weighted Loss (Tr·ªçng s·ªë m·∫•t m√°t)
```python
# T√≠nh class weights d·ª±a tr√™n ph√¢n b·ªë d·ªØ li·ªáu
class_weights = {
    'POSITIVE': 1.0,      # Ph·ªï bi·∫øn
    'NEGATIVE': 2.5-3.0,  # Hi·∫øm h∆°n ‚Üí tr·ªçng s·ªë cao
    'NEUTRAL': 5.0-8.0    # R·∫•t hi·∫øm ‚Üí tr·ªçng s·ªë r·∫•t cao
}

# S·ª≠ d·ª•ng trong loss function
loss = weighted_cross_entropy(pred, target, weights=class_weights)
```

#### B. Focal Loss (Cho hard examples)
```python
# T·∫≠p trung v√†o nh·ªØng m·∫´u kh√≥ ph√¢n lo·∫°i
focal_loss = -Œ± * (1 - p_t)^Œ≥ * log(p_t)
# Œ≥=2 th∆∞·ªùng t·ªët, t·∫≠p trung v√†o hard negatives
```

#### C. Oversampling Minority Classes
```python
# Nh√¢n ƒë√¥i/nh√¢n ba m·∫´u NEGATIVE v√† NEUTRAL trong training
# Ho·∫∑c d√πng: SMOTE, mixup, cutmix
```

### üéØ Chi·∫øn l∆∞·ª£c 2: C·∫£i Thi·ªán Tr√≠ch Xu·∫•t Th·ª±c Th·ªÉ

#### A. Character-level Encoding
```python
# Hi·ªán t·∫°i: token-level
# C·∫£i thi·ªán: character-level CNN

input: "battery life"  ‚Üí  char-level features
[b][a][t][t][e][r][y][ ][l][i][f][e]
        ‚Üì
     CNN filters
        ‚Üì
   better boundary detection
```

**L·ª£i √≠ch:** T·ª± ƒë·ªông h·ªçc multi-word entities, kh√¥ng ph·ª• thu·ªôc tokenization

#### B. Soft Attention over Entity Spans
```python
# Thay v√¨ hard selection, d√πng attention weights
# Cho ph√©p model h·ªçc linh ho·∫°t h∆°n
attention_weights = softmax(score)  # 0.0 - 1.0
entity_repr = sum(attention_weights * token_reps)
```

### üéØ Chi·∫øn l∆∞·ª£c 3: T·∫≠n D·ª•ng Th√¥ng Tin C√∫ Ph√°p

Hi·ªán t·∫°i code ƒë√£ d√πng dependency parsing (T·ªêT!), nh∆∞ng c√≥ th·ªÉ m·ªü r·ªông:

```python
# L·∫•y syntactic path gi·ªØa aspect v√† opinion
# VD: "The battery is very good"
#
#     battery ---nsubj---> is ---xcomp---> good
#
# Path: [battery] ---(nsubj)---> [is] ---(xcomp)---> [good]
# D√πng path n√†y ƒë·ªÉ guide model

syntax_path_features = extract_dependency_path(aspect, opinion, tree)
```

### üéØ Chi·∫øn l∆∞·ª£c 4: Data Augmentation

#### A. Aspect/Opinion Paraphrasing
```
Original:  "The battery is good"  ‚Üí (battery, good, POSITIVE)
Paraphrase: "The battery is great" ‚Üí (battery, great, POSITIVE)
Paraphrase: "The power cell is nice" ‚Üí (power cell, nice, POSITIVE)
```

#### B. Swap Polarity (C·∫©n th·∫≠n!)
```
Original:  "The battery is good" ‚Üí (battery, good, POSITIVE)
Negation:  "The battery is not bad" ‚Üí (battery, bad, NEGATIVE) ‚ùå
           "The battery is terrible" ‚Üí (battery, terrible, NEGATIVE) ‚úì
```

#### C. Back-Translation
```
English:  "The screen is beautiful"
French:   "L'√©cran est magnifique"
English:  "The display is magnificent"
```

### üéØ Chi·∫øn l∆∞·ª£c 5: Joint Learning (H·ªçc ƒë·ªìng th·ªùi)

```python
# Thay v√¨ ri√™ng l·∫ª:
# Entity extraction ‚Üí Sentiment classification

# D√πng Multi-task Learning:
#                    ‚îå‚îÄ Entity Decoder ‚îÄ‚îê
# Shared Encoder ‚îÄ‚Üí ‚îú Relation Decoder ‚îú‚Üí Final Triplet
#                    ‚îî‚îÄSentiment Decoder‚îò

# L·ª£i √≠ch:
# 1. Encoder h·ªçc shared representations
# 2. C√°c task h·ªó tr·ª£ l·∫´n nhau
# 3. Gi·∫£m overfitting
```

### üéØ Chi·∫øn l∆∞·ª£c 6: Curriculum Learning

```
Epoch 1-10:   H·ªçc tr√™n sentences v·ªõi 1-2 triplet
Epoch 11-20:  H·ªçc tr√™n sentences v·ªõi 2-4 triplet
Epoch 21+:    H·ªçc tr√™n to√†n b·ªô data (5+ triplet)

L·ª£i √≠ch: Model h·ªçc gradually, kh√¥ng overwhelmed
```

---

## 4. ƒêI·ªÇM C·ª§ TH·ªÇ THEO T·ª™ng DATASET

### 14lap (Laptop Reviews)
- **ƒê·∫∑c ƒëi·ªÉm:** C√¢n b·∫±ng nh·∫•t (POSITIVE: 57.5%)
- **ƒêi·ªÉm y·∫øu:** NEUTRAL hi·∫øm (9.6%)
- **G·ª£i √Ω:** T·∫≠p trung c·∫£i thi·ªán NEGATIVE (33%)

### 14res/15res/16res (Restaurant Reviews)
- **ƒê·∫∑c ƒëi·ªÉm:** C·ª±c k·ª≥ l·ªách v·ªÅ POSITIVE (73-75%)
- **ƒêi·ªÉm y·∫øu:** NEUTRAL r·∫•t hi·∫øm (3.5-7.3%)
- **G·ª£i √Ω:** Urgent need for oversampling NEGATIVE/NEUTRAL

---

## 5. CROSS-DOMAIN EXPERIMENT (Th·ª≠ nghi·ªám li√™n mi·ªÅn)

### √ù t∆∞·ªüng
```
Train on: 14res + 15res + 16res (nh√† h√†ng)
Test on:  14lap (laptop)

Ho·∫∑c reverse:
Train on: 14lap (laptop)
Test on:  14res (nh√† h√†ng)

Ki·ªÉm tra: Model c√≥ generalize sang domain m·ªõi kh√¥ng?
```

**K·∫øt qu·∫£ d·ª± ƒëo√°n:**
- N·∫øu t·ªët ‚Üí Model h·ªçc ƒë∆∞·ª£c syntactic/semantic patterns chung
- N·∫øu k√©m ‚Üí C·∫ßn domain adaptation techniques

---

## 6. ERROR ANALYSIS FRAMEWORK

```python
def analyze_errors():
    errors = {
        'false_positive': [],    # D·ª± ƒëo√°n nh∆∞ng kh√¥ng c√≥
        'false_negative': [],    # B·ªè l·ª°
        'sentiment_wrong': [],   # ƒê√∫ng triplet nh∆∞ng sentiment sai
        'span_wrong': [],        # Sentiment ƒë√∫ng nh∆∞ng span sai
    }
    
    for pred, gold in predictions:
        if pred != gold:
            # Ph√¢n lo·∫°i l·ªói
            categorize_error(pred, gold, errors)
    
    # Ph√¢n t√≠ch theo:
    # 1. Sentiment type (POSITIVE vs NEGATIVE vs NEUTRAL)
    # 2. Entity length (1-token vs multi-token)
    # 3. Distance (close vs far aspect-opinion)
    # 4. Sentence length
    
    return errors
```

---

## 7. TRI·ªÇN KHAI G·ª¢I √ù

### ∆Øu ti√™n cao (High Priority)
1. ‚úÖ **Implement weighted loss** ‚Üí +2-3% F1
2. ‚úÖ **Fix class imbalance** ‚Üí +3-5% F1
3. ‚úÖ **Error analysis** ‚Üí X√°c ƒë·ªãnh bottleneck th·ª±c t·∫ø

### ∆Øu ti√™n trung (Medium Priority)
4. **Focal loss** ‚Üí +1-2% F1
5. **Character-level encoding** ‚Üí +1-2% F1
6. **Data augmentation** ‚Üí +2-4% F1

### ∆Øu ti√™n th·∫•p (Low Priority)
7. **Curriculum learning** ‚Üí +0.5-1% F1
8. **Joint learning** ‚Üí Ph·ª©c t·∫°p, gain kh√¥ng r√µ

---

## 8. METRICS C·∫¶N THEO D√ïI

```python
# Kh√¥ng ch·ªâ micro F1, m√† c·∫£:
metrics = {
    'overall_f1': calculate_f1(all_preds, all_golds),
    
    'f1_positive': calculate_f1(positive_preds, positive_golds),
    'f1_negative': calculate_f1(negative_preds, negative_golds),
    'f1_neutral': calculate_f1(neutral_preds, neutral_golds),
    
    'f1_short_entities': calculate_f1_for_length_range(1, 2),
    'f1_long_entities': calculate_f1_for_length_range(3, 100),
    
    'f1_close_pairs': calculate_f1_for_distance_range(1, 5),
    'f1_far_pairs': calculate_f1_for_distance_range(6, 100),
}
```

---

## üìù T√ìM T·∫ÆT H√ÄNH ƒê·ªòNG

| # | Action | Effort | Expected Gain | Timeline |
|---|--------|--------|---------------|----------|
| 1 | Weighted Loss | ‚≠ê | ‚≠ê‚≠ê‚≠ê | 1-2h |
| 2 | Oversampling | ‚≠ê | ‚≠ê‚≠ê | 1h |
| 3 | Error Analysis | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 2-3h |
| 4 | Focal Loss | ‚≠ê | ‚≠ê‚≠ê | 1h |
| 5 | Char-level CNN | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | 4-6h |
| 6 | Data Augmentation | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 3-4h |
| 7 | Curriculum Learning | ‚≠ê‚≠ê‚≠ê | ‚≠ê | 3-4h |
| 8 | Joint Learning | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | 6-8h |

---

**Created:** 2026-01-29
**Analysis scripts:** `data_analysis.py`, `advanced_analysis.py`
