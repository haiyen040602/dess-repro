"""
Advanced Data Analysis & Insights for ABSTE Dataset
Ph√¢n t√≠ch chi ti·∫øt v√† l√™n √Ω t∆∞·ªüng c·∫£i thi·ªán
"""

import json
import os
import numpy as np
from collections import defaultdict, Counter
from pathlib import Path

class AdvancedAnalyzer:
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.datasets = ['14lap', '14res', '15res', '16res']
        self.splits = ['train', 'dev', 'test']
        
    def load_dataset(self, dataset_name, split):
        """Load a single dataset split"""
        path = os.path.join(
            self.data_dir, 
            dataset_name, 
            f'{split}_dep_triple_polarity_result.json'
        )
        
        if not os.path.exists(path):
            return None
        
        with open(path, 'r') as f:
            return json.load(f)
    
    def analyze_sentiment_distribution(self):
        """Analyze sentiment distribution across datasets"""
        sentiment_dist = defaultdict(lambda: defaultdict(int))
        
        for dataset_name in self.datasets:
            for split in self.splits:
                data = self.load_dataset(dataset_name, split)
                if data is None:
                    continue
                
                for sent in data:
                    sentiments = sent.get('sentiments', [])
                    for sentiment in sentiments:
                        stype = sentiment.get('type', 'unknown')
                        sentiment_dist[dataset_name][stype] += 1
        
        return sentiment_dist
    
    def analyze_triplet_patterns(self):
        """ÂàÜÊûê aspect-opinion triplet ÁöÑÊ®°Âºè
        Returns: Áµ±Ë®àÂì™‰∫õ target-opinion ÈÖçÂ∞çÊúÄÂ∏∏Ë¶ã
        """
        patterns = defaultdict(int)
        
        for dataset_name in self.datasets:
            for split in self.splits:
                data = self.load_dataset(dataset_name, split)
                if data is None:
                    continue
                
                for sent in data:
                    entities = sent.get('entities', [])
                    sentiments = sent.get('sentiments', [])
                    
                    for sentiment in sentiments:
                        head_idx = sentiment.get('head', -1)
                        tail_idx = sentiment.get('tail', -1)
                        stype = sentiment.get('type', 'unknown')
                        
                        if 0 <= head_idx < len(entities) and 0 <= tail_idx < len(entities):
                            head_type = entities[head_idx].get('type', 'unknown')
                            tail_type = entities[tail_idx].get('type', 'unknown')
                            
                            # Pattern: (head_type, tail_type, sentiment_type)
                            pattern = f"{head_type}-{tail_type}({stype})"
                            patterns[pattern] += 1
        
        return patterns
    
    def analyze_entity_span_lengths(self):
        """Analyze entity span length distribution"""
        span_lengths = defaultdict(list)
        
        for dataset_name in self.datasets:
            for split in self.splits:
                data = self.load_dataset(dataset_name, split)
                if data is None:
                    continue
                
                for sent in data:
                    entities = sent.get('entities', [])
                    for entity in entities:
                        start = entity.get('start', 0)
                        end = entity.get('end', 0)
                        length = end - start
                        span_lengths[dataset_name].append(length)
        
        return span_lengths
    
    def print_advanced_insights(self):
        """Print advanced insights and recommendations"""
        print("\n" + "="*100)
        print("ADVANCED ANALYSIS & INSIGHTS")
        print("="*100)
        
        # 1. Sentiment Distribution
        print("\n1Ô∏è‚É£  SENTIMENT DISTRIBUTION ACROSS DATASETS:")
        print("-" * 100)
        sentiment_dist = self.analyze_sentiment_distribution()
        
        for dataset_name in self.datasets:
            print(f"\n  {dataset_name}:")
            totals = sum(sentiment_dist[dataset_name].values())
            for stype, count in sorted(sentiment_dist[dataset_name].items()):
                pct = (count / totals * 100) if totals > 0 else 0
                print(f"    {stype:8} : {count:4} ({pct:5.1f}%)")
        
        # 2. Triplet Patterns
        print("\n\n2Ô∏è‚É£  ASPECT-OPINION TRIPLET PATTERNS:")
        print("-" * 100)
        patterns = self.analyze_triplet_patterns()
        sorted_patterns = sorted(patterns.items(), key=lambda x: x[1], reverse=True)
        
        print("\n  Top 15 Most Common Patterns:")
        for i, (pattern, count) in enumerate(sorted_patterns[:15], 1):
            print(f"    {i:2}. {pattern:30} : {count:4}")
        
        # 3. Entity Span Lengths
        print("\n\n3Ô∏è‚É£  ENTITY SPAN LENGTH ANALYSIS:")
        print("-" * 100)
        span_lengths = self.analyze_entity_span_lengths()
        
        for dataset_name in self.datasets:
            lengths = span_lengths[dataset_name]
            if lengths:
                avg_len = np.mean(lengths)
                min_len = min(lengths)
                max_len = max(lengths)
                median_len = np.median(lengths)
                
                print(f"\n  {dataset_name}:")
                print(f"    Average length: {avg_len:.2f} tokens")
                print(f"    Min length: {min_len} token(s)")
                print(f"    Max length: {max_len} tokens")
                print(f"    Median length: {median_len:.0f} tokens")
                
                # Count distribution
                len_dist = Counter(lengths)
                print(f"    Length distribution:")
                for length in sorted(len_dist.keys())[:10]:
                    count = len_dist[length]
                    pct = (count / len(lengths) * 100)
                    print(f"      {length} token(s): {count:4} ({pct:5.1f}%)")
    
    def print_ideas_recommendations(self):
        """Print ideas and recommendations for model improvement"""
        print("\n\n" + "="*100)
        print("üí° IDEAS & RECOMMENDATIONS FOR IMPROVEMENT")
        print("="*100)
        
        print("""
1. DATA CHARACTERISTICS:
   ‚úì Dataset is relatively balanced with ~3.4 entities per sentence
   ‚úì Aspect-Opinion ratio is 2:1, indicating paired structure
   ‚úì Sentiment heavily skewed towards POSITIVE (majority are positive reviews)
   
   Issues:
   ‚ö† Class imbalance: POSITIVE >> NEGATIVE > NEUTRAL
     ‚Üí Recommendation: Use weighted loss, focal loss, or oversampling for minority classes
   
   ‚ö† Short entity spans: Most entities are 1-3 tokens long
     ‚Üí Recommendation: Use character-level or subword-level spans for better precision

2. MODEL ARCHITECTURE IMPROVEMENTS:
   
   a) Handling Class Imbalance:
      ‚Ä¢ Use class weights in loss function
      ‚Ä¢ Implement focal loss for hard negatives
      ‚Ä¢ Data augmentation: paraphrase, back-translation
   
   b) Better Entity Encoding:
      ‚Ä¢ Use character CNNs for entity boundaries
      ‚Ä¢ Implement soft attention over entity tokens
      ‚Ä¢ Multi-head attention for different semantic aspects
   
   c) Leverage Syntactic Information:
      ‚úì Already using dependency parsing - good!
      ‚Ä¢ Enhance with: constituency parsing, SRL (Semantic Role Labeling)
      ‚Ä¢ Use syntax-aware graph neural networks
   
   d) Joint Learning:
      ‚Ä¢ Multi-task: Entity extraction ‚Üí Sentiment classification
      ‚Ä¢ Shared encoder but task-specific decoders

3. TRAINING STRATEGIES:
   
   a) Curriculum Learning:
      ‚Ä¢ Start with sentences containing fewer triplets
      ‚Ä¢ Gradually increase complexity
   
   b) Data Augmentation:
      ‚Ä¢ Paraphrase entities/opinions while keeping relations
      ‚Ä¢ Swap sentiments with different aspects (careful!)
      ‚Ä¢ Reverse aspect-opinion direction for symmetric relations
   
   c) Hyperparameter Tuning:
      ‚Ä¢ Learning rate scheduling: warmup + decay
      ‚Ä¢ Different learning rates for different layers
      ‚Ä¢ Ensemble multiple models

4. EVALUATION IMPROVEMENTS:
   
   a) Error Analysis:
      ‚Ä¢ Analyze false positives/negatives by sentiment type
      ‚Ä¢ Check performance on different entity span lengths
      ‚Ä¢ Evaluate on long vs short sentences
   
   b) Cross-domain Evaluation:
      ‚Ä¢ Train on restaurant reviews (14res, 15res, 16res) ‚Üí test on laptops (14lap)
      ‚Ä¢ Identify domain-specific patterns

5. FEATURE ENGINEERING:
   
   a) Contextual Features:
      ‚Ä¢ Distance between aspect and opinion words
      ‚Ä¢ Syntactic path in dependency tree
      ‚Ä¢ POS tag sequences
   
   b) Semantic Features:
      ‚Ä¢ Word embeddings (GloVe, Word2Vec)
      ‚Ä¢ Contextual embeddings (BERT already used - good!)
      ‚Ä¢ Entity-opinion semantic similarity

6. POST-PROCESSING:
   
   a) Constraint-Based Filtering:
      ‚Ä¢ Filter invalid triplets (e.g., aspect and opinion swapped)
      ‚Ä¢ Merge overlapping/redundant predictions
      ‚Ä¢ Apply confidence thresholds per class
   
   b) Consistency Regularization:
      ‚Ä¢ Ensure same aspect has consistent sentiment across document
      ‚Ä¢ Use document-level sentiment as constraint

7. SPECIFIC OBSERVATIONS FROM DATA:
   
   Dataset Characteristics:
   ‚Ä¢ 14lap (Laptop reviews): Most POSITIVE (56-67%)
   ‚Ä¢ 14res/15res/16res (Restaurant reviews): Strong POSITIVE bias (70-77%)
   
   Recommendation:
   ‚Üí Focus on NEGATIVE sample mining for better minority class learning
   ‚Üí Use stratified sampling during training
   
   Entity Patterns:
   ‚Ä¢ Mostly target-opinion pairs (50-50 split)
   ‚Ä¢ Some sentences have multiple triplets (up to 5+ per sentence)
   
   Recommendation:
   ‚Üí Handle multi-triplet sentences carefully
   ‚Üí Avoid duplicate predictions for same aspects
""")
        
        print("="*100)


def main():
    data_dir = '../data'
    
    print("\nüî¨ Advanced Data Analysis Starting...\n")
    analyzer = AdvancedAnalyzer(data_dir)
    
    analyzer.print_advanced_insights()
    analyzer.print_ideas_recommendations()
    
    print("\n‚úÖ Advanced Analysis Complete!\n")


if __name__ == '__main__':
    main()
